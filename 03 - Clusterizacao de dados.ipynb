{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55d76575-be45-43e0-99ef-c8595a933353",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "585ba14b-5a26-4149-ab73-9d8273147390",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = 'dbfs:/FileStore/siver/dados.parquet/'\n",
    "df_data = ps.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d2fec9-6df4-4093-8b71-7126a063fd6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.pandas.frame.DataFrame'>\nInt64Index: 170653 entries, 0 to 170652\nData columns (total 19 columns):\n #   Column            Non-Null Count   Dtype  \n---  ------            --------------   -----  \n 0   valence           170653 non-null  float64\n 1   year              170653 non-null  int32  \n 2   acousticness      170653 non-null  float64\n 3   artists           170653 non-null  object \n 4   danceability      170080 non-null  float64\n 5   duration_ms       170454 non-null  int64  \n 6   energy            170573 non-null  float64\n 7   explicit          170606 non-null  int64  \n 8   id                170653 non-null  object \n 9   instrumentalness  170270 non-null  float64\n 10  key               170451 non-null  int64  \n 11  liveness          170613 non-null  float64\n 12  loudness          170621 non-null  float64\n 13  mode              170635 non-null  int64  \n 14  name              170653 non-null  object \n 15  popularity        169496 non-null  int64  \n 16  release_date      170653 non-null  object \n 17  speechiness       170042 non-null  float64\n 18  tempo             170404 non-null  float64\ndtypes: float64(9), int32(1), int64(5), object(4)"
     ]
    }
   ],
   "source": [
    "df_data.dropna()\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5f82d79-ac5a-45c7-a127-6ea15fa931d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "  release_date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f9fc0e-2131-4cce-8b29-71b35b53e9b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: Index(['valence', 'year', 'acousticness', 'artists', 'danceability',\n       'duration_ms', 'energy', 'explicit', 'id', 'instrumentalness', 'key',\n       'liveness', 'loudness', 'mode', 'name', 'popularity', 'release_date',\n       'speechiness', 'tempo'],\n      dtype='object')"
     ]
    }
   ],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0558459-21a3-4115-abe3-30d9207bed34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_data['artists_song'] = df_data.artists+' - '+df_data.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af731dc0-29aa-495e-aaf7-f5833e3a1e5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: ['valence',\n 'year',\n 'acousticness',\n 'danceability',\n 'duration_ms',\n 'energy',\n 'explicit',\n 'instrumentalness',\n 'key',\n 'liveness',\n 'loudness',\n 'mode',\n 'popularity',\n 'speechiness',\n 'tempo']"
     ]
    }
   ],
   "source": [
    "X = df_data.columns.to_list()\n",
    "X.remove('artists')\n",
    "X.remove('id')\n",
    "X.remove('name')\n",
    "X.remove('artists_song')\n",
    "X.remove('release_date')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5553a21-8b55-4968-9f00-a058482d0755",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_data = df_data.to_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c873bbd5-a70f-453a-b1cc-76e1cb3f7c84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1024843900675777>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m dados_encoded_vector \u001B[38;5;241m=\u001B[39m \u001B[43mVectorAssembler\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputCols\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutputCol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfeatures\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_data\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:262\u001B[0m, in \u001B[0;36mTransformer.transform\u001B[0;34m(self, dataset, params)\u001B[0m\n",
       "\u001B[1;32m    260\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_transform(dataset)\n",
       "\u001B[1;32m    261\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be a param map but got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params))\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:400\u001B[0m, in \u001B[0;36mJavaTransformer._transform\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    397\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    399\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n",
       "\u001B[0;32m--> 400\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mtransform(\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m), dataset\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'functools.partial' object has no attribute '_jdf'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-1024843900675777>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m dados_encoded_vector \u001B[38;5;241m=\u001B[39m \u001B[43mVectorAssembler\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputCols\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutputCol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfeatures\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_data\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:262\u001B[0m, in \u001B[0;36mTransformer.transform\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_transform(dataset)\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be a param map but got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params))\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:400\u001B[0m, in \u001B[0;36mJavaTransformer._transform\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    397\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[0;32m--> 400\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mtransform(\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m), dataset\u001B[38;5;241m.\u001B[39msparkSession)\n\n\u001B[0;31mAttributeError\u001B[0m: 'functools.partial' object has no attribute '_jdf'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'functools.partial' object has no attribute '_jdf'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dados_encoded_vector = VectorAssembler(inputCols=X, outputCol='features').transform(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf78898f-81e0-490d-a2d9-92aa34b5f21f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03 - Clusterizacao de dados",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
